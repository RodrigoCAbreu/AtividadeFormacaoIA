{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "eR0nQG-Uf0Kh",
    "outputId": "b5d636fc-5989-4fe4-fe34-e1e9660c5557"
   },
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Processamento de Linguagem Natural</font>\n",
    "\n",
    "\n",
    "## Mini-Projeto 3\n",
    "### Reconhecimento da Fala - Detectando Emoções em Arquivos de Áudio com Inteligência Artificial\n",
    "### Parte 1 - Preparação dos Dados, Treinamento e Avaliação do Detector de Emoções com Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este é um Mini-Projeto especial. \n",
    "\n",
    "Vamos trabalhar com uma das tarefas mais complexas em Inteligência Artificial: extrair emoções a partir da voz em arquivos de áudio.\n",
    "\n",
    "Detectar emoções é uma das estratégias de Marketing mais importantes no mundo de hoje. Você pode personalizar aplicações para fornecer tratamentos diferentes para um indivíduo de acordo com a emoção detectada na voz de uma pessoa. Esse tipo de aplicação é um dos pilares para uma solução completa de IA.\n",
    "\n",
    "Alguns exemplos de aplicações desse tipo de solução, incluem:\n",
    "\n",
    "- Uma central de atendimento que toca músicas diferentes de acordo com a emoção detectada na voz do cliente. \n",
    "\n",
    "- Um carro autônomo que desacelera quando alguém está com raiva ou com medo. \n",
    "\n",
    "- Assistente pessoal que reage de acordo com a emoção detectada na voz.\n",
    "\n",
    "- Aplicações de Marketing que oferecem diferentes produtos ou opções de acordo com a emoção do cliente.\n",
    "\n",
    "- Assistente Virtual, que pode ser uma Enfermeira Virtual ou mesmo um Professor de Inglês, e que reage de acordo com a voz do interlocutor.\n",
    "\n",
    "Entre outros exemplos.\n",
    "\n",
    "Usaremos a biblioteca Librosa em Python para processar e extrair recursos dos arquivos de áudio. Librosa é um pacote Python para análise de música e áudio. Ele fornece os componentes necessários para criar sistemas de recuperação de informações musicais. \n",
    "\n",
    "Usando a biblioteca librosa, conseguimos extrair recursos através do MFCC (Mel Frequency Cepstral Coefficient). Os MFCCs são coeficientes amplamente usado no reconhecimento automático de fala. \n",
    "\n",
    "Também separamos a voz de mulheres e homens usando os identificadores fornecidos no dataset, como forma de deixar o modelo de reconhecimento de voz ainda mais preciso e personalizado.\n",
    "\n",
    "Cada arquivo de áudio fornece muitos recursos, que são basicamente uma matriz de muitos valores. A esses recursos, iremos atribuir rótulos especificando o gênero da voz no áudio e a emoção detectada.\n",
    "\n",
    "Como este projeto é um grande esforço de trabalho, ele foi dividido em 3 partes:\n",
    "\n",
    "- **Parte 1 - Preparação dos Dados, Treinamento e Avaliação do Detector de Emoções com Machine Learning**\n",
    "- **Parte 2 - Preparação dos Dados, Treinamento e Avaliação do Detector de Emoções com Deep Learning**\n",
    "- **Parte 3 - Detecção e Classificação de Emoções em Arquivos de Áudio**\n",
    "\n",
    "Este projeto pode ser facilmente adaptado aos seus próprios projetos. Tudo que você precisa é providenciar arquivos de áudio gravados com pessoas com diferentes emoções. Usaremos um dataset público para nosso trabalho.\n",
    "\n",
    "Esta é a Parte 1. Aprenda e divirta-se."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Para construir nossa aplicação, usaremos arquivos de áudio disponíveis publicamente neste web site:\n",
    "\n",
    "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)\n",
    "\n",
    "https://zenodo.org/record/1188976\n",
    "\n",
    "Faça download do arquivo **Audio_Song_Actors_01-24.zip** (225 MB). Descompacte e coloque o conteúdo (24 pastas) dentro do diretório **dados**, no mesmo diretório onde está este Jupyter Notebook.\n",
    "\n",
    "São 1440 arquivos de áudio com 60 falas por ator x 24 atores = 1440.   \n",
    "\n",
    "Caso tenha dificuldades com o download, disponibilizamos o arquivo no Titan, na pasta: /media/datasets/PLN/Cap03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organização dos Dados\n",
    "\n",
    "Cada arquivo de áudio tem o seguinte formato: '03-02-03-02-02-02-01.wav'\n",
    "\n",
    "Os pares de valores representam uma informação definida no Encoding abaixo:\n",
    "\n",
    "- Modalidade (01 = AV total, 02 = somente vídeo, 03 = somente áudio).\n",
    "- Canal vocal (01 = fala, 02 = música).\n",
    "- Emoção (01 = neutro, 02 = calmo, 03 = feliz, 04 = triste, 05 = zangado, 06 = medroso, 07 = nojo, 08 = surpreso).\n",
    "- Intensidade emocional (01 = normal, 02 = forte). Obs: Não há intensidade forte para a emoção \"neutra\".\n",
    "- Declaração (01 = \"As crianças estão conversando na porta\", 02 = \"Os cães estão sentados na porta\").\n",
    "- Repetição (01 = 1ª repetição, 02 = 2ª repetição).\n",
    "- Ator (01 a 24. Os atores ímpares são homens, os atores pares são mulheres).\n",
    "\n",
    "As falas são em inglês, mas são sempre as mesmas frases, ditas por atores homens e mulheres e com diferentes emoções e intensidades.\n",
    "\n",
    "Por exemplo. Considere o arquivo: '03-01-06-01-02-01-12.wav'\n",
    "\n",
    "Esse arquivo é:\n",
    "\n",
    "- Somente áudio (03) \n",
    "- Discurso (01) \n",
    "- Medo (06) \n",
    "- Intensidade normal (01) \n",
    "- Declaração \"cães\" (02) \n",
    "- 1ª repetição (01) \n",
    "- 12º ator (12) Mulher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns termos comuns relacionados ao som:\n",
    "\n",
    "**Audio Frame**\n",
    "\n",
    "Um frame de áudio, ou amostra, contém informações de amplitude (volume) naquele momento específico. Para produzir som, dezenas de milhares de frames são reproduzidos em sequência para produzir frequências.\n",
    "\n",
    "**Bit Rate**\n",
    "\n",
    "Refere-se à qualidade do fluxo do áudio. É medido em Kilobitspersec (kbps ou k). A taxa de bits não é de bits (dados) codificados por segundo ou o não, mas de bits transmitidos ou recebidos por segundo. Maior taxa de bits com mais taxa de amostragem, requer alta largura de banda e produz boa qualidade de áudio.\n",
    "\n",
    "**Taxa de Amostragem (Sampling Rate)**\n",
    "\n",
    "Taxa de amostragem (às vezes chamada frequência de amostragem ou Fs) é o número de pontos de dados adquiridos por segundo. Uma taxa de amostragem de 2000 amostras / segundo significa que 2000 pontos de dados discretos são adquiridos a cada segundo. Isso pode ser chamado de frequência de amostra de 2000 Hertz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala pacote librosa\n",
    "!pip install -q librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala pacote xgboost\n",
    "!pip install -q xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4shbaHxJiq6q"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as lr\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import seaborn as sns\n",
    "import xgboost\n",
    "import sklearn\n",
    "from glob import glob\n",
    "from joblib import dump, load\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S8vHLrlD-W-M"
   },
   "source": [
    "## Carregando os Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acesse o site do Projeto The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS):\n",
    "\n",
    "https://zenodo.org/record/1188976\n",
    "\n",
    "Faça download do arquivo **Audio_Song_Actors_01-24.zip** (225 MB). Descompacte e coloque o conteúdo (24 pastas) dentro do diretório **dados**, no mesmo diretório onde está este Jupyter Notebook.\n",
    "\n",
    "São 1440 arquivos de áudio com 60 áudios por ator x 24 atores = 1440.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qZRgmEOtob7v"
   },
   "outputs": [],
   "source": [
    "# Executaremos um loop em 24 pastas. O nome das pastas segue o mesmo padrão:\n",
    "\n",
    "# Actor_01\n",
    "# Actor_24\n",
    "\n",
    "# Vamos checar o código ao final do nome da pasta para obter o caminho completo e \n",
    "# então coletar o nome de cada arquivo.\n",
    "\n",
    "# Define o diretório raiz onde estão os diretórios com os arquivos de áudio\n",
    "dir_raiz = 'dados'\n",
    "\n",
    "# Dicionários para receber os resultados do loop\n",
    "arquivos = {}\n",
    "taxa_amostragem = {}\n",
    "\n",
    "# Loop pelo diretório com os arquivos de áudio\n",
    "for itens in range(1,25):\n",
    "    if len(str(itens)) == 1:\n",
    "        audio_dir = dir_raiz + '/Actor_' + str('0') + str(itens)\n",
    "    else:\n",
    "        audio_dir = dir_raiz + '/Actor_' + str(itens)\n",
    "    \n",
    "    # Armazena o caminho para todos os arquivos de um diretório\n",
    "    audio_files = glob(audio_dir + '/*.wav')\n",
    "    \n",
    "    # Armazena o caminho e a taxa de amostragem de cada arquivo na pasta\n",
    "    for i in range(len(audio_files)):\n",
    "        x = audio_files[i]\n",
    "        audio, sfreq = lr.load(audio_files[i], sr = None)\n",
    "        arquivos[x] = len(audio) / sfreq\n",
    "        taxa_amostragem[x] = sfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém as taxas de amostragem dos arquivos\n",
    "[taxa_amostragem[i] for i in list(taxa_amostragem.keys())[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica se todas as taxas de amostragem são iguais\n",
    "all(value == 48000 for value in taxa_amostragem.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um dataframe para receber a lista de caminhos dos arquivos\n",
    "arquivos_audio = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o dataframe\n",
    "for keys, values in arquivos.items():\n",
    "    arquivos_audio.at[keys,'lengthoffile'] = values   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza os dados\n",
    "arquivos_audio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UM_wmcPjoplE"
   },
   "outputs": [],
   "source": [
    "# Cria uma função para retornar os arquivos de áudio e a taxa de amostragem de cada arquivo\n",
    "def extrai_audio_data(file):\n",
    "    audio, sfreq = lr.load(file, sr = None)\n",
    "    return audio, sfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos verificar o shape dos arquivos de áudio e taxa de amostragem\n",
    "audio, sfreq = extrai_audio_data(arquivos_audio.index[0])\n",
    "print(f'\\nShape da Série Representando o Áudio (y): {audio.shape} \\nTaxa de Amostragem (sr): {sfreq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>y é a série temporal representando o áudio e tem o formato: np.ndarray [shape=(n,)]</li>\n",
    "\n",
    "<li>sr é o sampling rate de y, tendo o formato: number > 0 [scalar]</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora preparamos o dataset para exploração e treinamento do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> MFCC </b>\n",
    "\n",
    "O primeiro passo em qualquer sistema de reconhecimento automático de fala é extrair recursos, ou seja, identificar os componentes do sinal de áudio que são bons para identificar o conteúdo linguístico e descartar todas as outras coisas que carregam informações como ruído de fundo, interferências, etc.\n",
    "\n",
    "O ponto principal a entender sobre a fala é que os sons gerados por um ser humano são filtrados pelo formato do trato vocal, incluindo língua, dentes, etc. Esse formato determina qual som é produzido. Se pudermos determinar a forma com precisão, isso deve nos dar uma representação precisa do fonema que está sendo produzido. A forma do trato vocal se manifesta no espectro de potência de curto espaço de tempo, e o trabalho dos MFCCs é representar com precisão esse espectro. \n",
    "\n",
    "Os Coeficientes Cepstrais de Frequência Mel (MFCCs) são um recurso amplamente usado no reconhecimento automático de fala e alto-falante. Eles foram introduzidos por Davis e Mermelstein na década de 1980 e têm sido o estado da arte desde então. Antes da introdução dos MFCCs, os Coeficientes de Previsão Linear (LPCs) e os Coeficientes Cepstrais de Previsão Linear (LPCCs) eram o principal tipo de recurso para reconhecimento automático de fala, especialmente com classificadores HMM. \n",
    "\n",
    "A escala mel, com nome derivado da palavra melodia (melody), é uma escala logarítmica perceptual deﬁnida por (STEVENS; VOLKMANN; NEWMAN,1937) que tem por objetivo manter os tons de frequência equidistantes tomando como referência 40 dB acima do limite de percepção humana em 1000 Hz. \n",
    "\n",
    "Os Coeficientes de Frequência Mel-Cepstrais (MFCC - Mel Frequency Cepstral Coefficients) são calculados sobre uma janela, isto é, número de amostras. O som é onda e não se pode derivar nenhum recurso colhendo uma única amostra (número), daí a janela. Na prática, nosso conjunto de dados de áudio será tratado como ums série temporal.\n",
    "\n",
    "![title](imagens/mfcc.jpeg)\n",
    "\n",
    "Para calcular o MFCC, a transformação rápida de Fourier (FFT - Fast Fourier Transform) é usada e isso exige exatamente que o comprimento de uma janela seja fornecido. Na biblioteca librosa, os itens abaixo estão implícitos especificamente:\n",
    "\n",
    "- Comprimento da janela da FFT: 2048 \n",
    "- Número de amostras entre frames sucessivos: 512 \n",
    "\n",
    "Se um coeficiente cepstral tem um valor positivo, ele representa um som sonorante, pois a maioria da energia espectral nos sons sonorantes está concentrada nas regiões de baixa frequência.\n",
    "\n",
    "Por outro lado, se um coeficiente cepstral tem um valor negativo, ele representa um som fricativo, pois a maioria das energias espectrais nos sons fricativos está concentrada em altas frequências.\n",
    "\n",
    "Os coeficientes de ordem inferior contêm a maioria das informações sobre a forma espectral geral da função de transferência do filtro de fonte.\n",
    "\n",
    "O coeficiente de ordem zero indica a potência média do sinal de entrada.\n",
    "\n",
    "O coeficiente de primeira ordem representa a energia espectral de distribuição entre baixas e altas frequências.\n",
    "\n",
    "O reconhecimento da fala com IA passa pela compreensão de como funciona o som e a voz e separamos esses materiais abaixo. Recomendamos a leitura antes de prosseguir:\n",
    "\n",
    "<a href=\"https://www.researchgate.net/publication/272351208_Componentes_Mel_Cepstrais\">Componentes Mel Cepstrais</a>\n",
    "\n",
    "<a href=\"http://www.cs.columbia.edu/~julia/courses/CS6998-2019/%5B09%5D%20Automatic%20Speech%20Recognition.pdf\">Automatic Speech Recognition</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma função para extrair o MFCC de um dado arquivo\n",
    "def mfcc_extract(file):\n",
    "    audio, sfreq = extrai_audio_data(file)\n",
    "    mfccs = librosa.feature.mfcc(audio, sr = sfreq)\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista vazia\n",
    "list_data_frame = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tuTpzYvDo-rv"
   },
   "outputs": [],
   "source": [
    "# Considere o arquivo: '03-01-06-01-02-01-12.wav'\n",
    "\n",
    "# Esse arquivo é:\n",
    "\n",
    "# Somente áudio(03) \n",
    "# Discurso(01) \n",
    "# Medo(06) \n",
    "# Intensidade normal(01) \n",
    "# Declaração \"cães\"(02) \n",
    "# 1ª repetição(01) \n",
    "# 12º ator(12) Mulher.\n",
    "\n",
    "# Vamos fazer um loop e extrair os detalhes acima de todos os arquivos.\n",
    "\n",
    "# Loop por cada arquivo\n",
    "for file_path in arquivos_audio.index:\n",
    "    \n",
    "    # Chamamos a função de extração mfcc para retornar coeficientes e dados mfcc\n",
    "    data = mfcc_extract(file_path)\n",
    "    \n",
    "    # Transformamos os dados no formato (n samples, n features)\n",
    "    frame = pd.DataFrame(data.T, columns = ['mfcc' + str(x) for x in range(0,20)])\n",
    "    \n",
    "    # Extraímos gênero do ator do arquivo usando o encoding\n",
    "    frame['ID_ATOR_SEXO'] = file_path[33:35]\n",
    "    \n",
    "    # Extraímos o código da emoção do arquivo usando o encoding\n",
    "    frame['ID_EMOCAO'] = file_path[21:23]\n",
    "    \n",
    "    # Extraímos a intensidade da emoção do arquivo usando o encoding\n",
    "    frame['ID_INTENSIDADE_EMOCIONAL'] = file_path[24:26]\n",
    "    \n",
    "    # Inclui na lista\n",
    "    list_data_frame.append(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora preparamos o dataframe final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatena os elementos da lista em um dataframe\n",
    "df_dados_audio = pd.concat(list_data_frame, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O dataset está quase pronto. \n",
    "# Precisamos apenas incluir os labels para compreender o que temos em cada arquivo de áudio.\n",
    "df_dados_audio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o label para o gênero da voz a partir do código de gênero, sendo ímpar para masculino e par para feminino\n",
    "df_dados_audio[\"LABEL_GENERO\"] = list(map(lambda x: 'homem' if int(x)%2 == 1 else 'mulher', \n",
    "                                        df_dados_audio.ID_ATOR_SEXO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label para a emoção\n",
    "df_dados_audio['LABEL_EMOCAO'] = df_dados_audio.ID_EMOCAO.map({'01':'neutro',\n",
    "                                                               '02':'calmo',\n",
    "                                                               '03':'feliz',\n",
    "                                                               '04':'triste',\n",
    "                                                               '05':'zangado',\n",
    "                                                               '06':'medo',\n",
    "                                                               '07':'nojo',\n",
    "                                                               '08':'surpreso'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intensidade emocional normal ou forte \n",
    "df_dados_audio['LABEL_INTENSIDADE'] = df_dados_audio.ID_INTENSIDADE_EMOCIONAL.map({'01':'normal',\n",
    "                                                                                   '02':'forte'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pESu_jhCwkQi"
   },
   "outputs": [],
   "source": [
    "# Gênero e emoção da voz no áudio\n",
    "df_dados_audio['LABEL_GENERO_EMOCAO'] = df_dados_audio['LABEL_GENERO'] + \"_\" + df_dados_audio['LABEL_EMOCAO']\n",
    "df_final = df_dados_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipos de Dados\n",
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando os dados\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando os dados\n",
    "df_final.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados estão prontos. Vamos fazer uma análise exploratória."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SmdzIWatsoj"
   },
   "source": [
    "## Análise Exploratória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "j_tvSc20t-XL",
    "outputId": "3135ec22-6c4d-47a4-8555-47afbda27c94"
   },
   "outputs": [],
   "source": [
    "# Verificando a distribuição de arquivos de áudio por gênero do ator\n",
    "sns.countplot(x = 'LABEL_GENERO', data = df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "VBmmiEsmt-aB",
    "outputId": "d8687d6b-9db6-429d-b119-495fe6b79862"
   },
   "outputs": [],
   "source": [
    "# Verificando a distribuição de arquivos de áudio por emoção do ator\n",
    "sns.countplot(x = 'LABEL_EMOCAO', data = df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "colab_type": "code",
    "id": "7-WmxnZ5t-cm",
    "outputId": "3b8f5e14-eeee-4f2a-aeb7-d8276b8f939c"
   },
   "outputs": [],
   "source": [
    "# Verificando a distribuição de arquivos de áudio por gênero/emoção do ator\n",
    "plt.figure(figsize = (12,4))\n",
    "sns.countplot(x = 'LABEL_GENERO_EMOCAO', data = df_final)\n",
    "plt.xticks(rotation = 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VaJY_flYy3mq"
   },
   "source": [
    "Vamos usar clusters K-Means para ver se clusters distintos podem ser formados com os dados e visualizar os clusters usando PCA em duas dimensões para escolher clusters ideais para descrever os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whl1PyRnAoAw"
   },
   "outputs": [],
   "source": [
    "# Vamos padronizar os dados e deixá-los na mesma escala. Criamos o objeto scaler:\n",
    "z_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos o objeto scaler\n",
    "# Observe que não aplicamos o scaler às variáveis categóricas que criamos anteriormente\n",
    "df_final_scaled = z_scaler.fit_transform(df_final[df_final.columns[:-7]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza uma amostra dos dados\n",
    "df_final_scaled[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1BdIOHbdAZTf"
   },
   "source": [
    "Para usar o algoritmo K-Means, precisamos escolher o melhor de K. Vamos usar o método elbow para isso, testando diversos valores de k. O valor de k representa o número ideal de clusters para agrupar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXFD_XLTABVL"
   },
   "outputs": [],
   "source": [
    "# Função para encontrar o melhor valor de k\n",
    "def busca_melhor_k(k = 2):\n",
    "    kmeans = KMeans(n_clusters = k, random_state = 0).fit(df_final_scaled)\n",
    "    return kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um dicionário para receber o resultado da busca\n",
    "dict_k = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "__99M-6sDt4k",
    "outputId": "8d96a30a-90f9-40f6-a7b4-a1cde1147d3b"
   },
   "outputs": [],
   "source": [
    "# Loop pelos valores de k pesquisados\n",
    "for i in range(2,10):\n",
    "    dict_k[i] = busca_melhor_k(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime o resultado\n",
    "for k, ss in dict_k.items() :\n",
    "    print(f'Para cada valor de k = {k}, Silhouette Coefficient = {ss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "colab_type": "code",
    "id": "4vyrQkP3FC4q",
    "outputId": "686c2c5f-6442-4635-e0b9-0037021124eb"
   },
   "outputs": [],
   "source": [
    "# Um Plot ajuda a visualizar melhor qual o valor de k ideal para a clusterizção\n",
    "plt.figure(figsize = (12,5))\n",
    "plt.plot(list(dict_k.keys()), list(dict_k.values()), marker = 'D')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Silhouette Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGMVsqqQLUJw"
   },
   "source": [
    "Como visto no gráfico de elbow acima, é muito difícil obter um bom valor para K. Vamos então usar PCA para visualizar diferentes clusters K-Means. Nosso objetivo é visualizar a organização e agrupamento dos dados, o que é bem complicado por conta da alta dimensionalidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HId6JIk4LfEU"
   },
   "source": [
    "Vamos usar PCA (Redução de Dimensionalidade) para visualizar os clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMfncvVI0URy"
   },
   "outputs": [],
   "source": [
    "# Lista de marcadores\n",
    "marker = ['.',',','o','v','^','<','>','1','2','3','4','8','s','p','P','*','h','H','+','x','X','D','d','|','_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objeto PCA com 2 componentes principais\n",
    "pca_obj = PCA(n_components = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para aplicar o PCA\n",
    "def aplica_pca(valor_k = 2):\n",
    "    \n",
    "    # Cria os clusters K-Means\n",
    "    clusters = KMeans(valor_k).fit_predict(df_final_scaled)\n",
    "    \n",
    "    # Aplica PCA\n",
    "    componentes = pca_obj.fit_transform(df_final_scaled)\n",
    "    \n",
    "    # Retorna o cluster e os dados transformados com PCA\n",
    "    return clusters, componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dos dados transformados com PCA\n",
    "def plot_pca(df, k):\n",
    "    plt.figure(figsize = (20,15))\n",
    "    _ = sns.lmplot(x = 'Primeiro Componente PCA', \n",
    "                   y = 'Segundo Componente PCA',\n",
    "                   data = df,\n",
    "                   hue = 'Cluster',\n",
    "                   fit_reg = False,\n",
    "                   legend = False,\n",
    "                   palette = \"Set2\",\n",
    "                   markers = marker[:k])\n",
    "    plt.title(f'Representação dos Clusters de Dados com PCA Para K = {k}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wLVy92eUNsAH",
    "outputId": "9059dc64-55be-4f59-bb12-079a9f20262c"
   },
   "outputs": [],
   "source": [
    "# Para diferentes valores de k de 2 a 10, criamos o cluster em cada k, transformamos os dados com PCA e criamos o plot\n",
    "for n_clusters in range(2, 10):\n",
    "    clusters, pca_transf = aplica_pca(n_clusters)\n",
    "    df_pca = pd.DataFrame({'Cluster':list(clusters),\n",
    "                           'Primeiro Componente PCA':pca_transf[:,0],\n",
    "                           'Segundo Componente PCA':pca_transf[:,1]})\n",
    "    plot_pca(df_pca, n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5BZHVj5KZYxg"
   },
   "source": [
    "Podemos ver pelos gráficos acima que existem alguns limites bem definidos, mesmo com maior número de clusters. Os dados estão bem organizados e podemos passar para a construção dos modelos de Machine Learning para criação do classificador para detectar emoções na voz dos arquivos de áudio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 1 - Prever o gênero do ator no áudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYC-4bsddKUE"
   },
   "source": [
    "Vamos criar o modelo para prever o sexo do ator no áudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dTJFe4-rbVuf"
   },
   "outputs": [],
   "source": [
    "# Gerando o valor de x (sem os labels)\n",
    "X = df_final[df_final.columns[:-7]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando Label Encoder para converter a string em representação numérica\n",
    "y = LabelEncoder().fit_transform(df_final['LABEL_GENERO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qj1w8oBncjzq"
   },
   "outputs": [],
   "source": [
    "# Divisão em dados de treino e de teste\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size = 0.33, stratify = y, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "X_treino.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "y_treino.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "X_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "y_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos o scaler nos dados de treino\n",
    "fitted = z_scaler.fit(X_treino)\n",
    "\n",
    "# E transformamos dados de treino e de teste\n",
    "X_treino = fitted.transform(X_treino)\n",
    "X_teste = fitted.transform(X_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UyGZBRlrdjx3"
   },
   "source": [
    "Criamos agora uma lista de classificadores para avaliar o desempenho de vários classificadores nos dados usando métricas como matriz de confusão e acurácia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "uEdHjLQ82Sg4",
    "outputId": "3938de2b-e6d0-44f7-e394-1e95270abe6f"
   },
   "outputs": [],
   "source": [
    "# Dicionário de algoritmos de classificação\n",
    "classificadores = {'XGboost':XGBClassifier(),\n",
    "                   'DecisonTree':DecisionTreeClassifier(),\n",
    "                   'RandomForest':RandomForestClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para treinar cada classificador de gênero e avaliar a performance \n",
    "def treina_avalia_genero(classificadores, X_treino, X_teste, y_treino, y_teste):\n",
    "    \n",
    "    # Loop pelo dicionário de classificadores\n",
    "    for k, clf in classificadores.items():\n",
    "        \n",
    "        print(\"\\nIniciando o Treinamento do Modelo \" + k + \"...\")\n",
    "        \n",
    "        # Treina o modelo\n",
    "        clf.fit(X_treino, y_treino)\n",
    "        \n",
    "        # Faz a previsão\n",
    "        y_pred = clf.predict(X_teste)\n",
    "        \n",
    "        # Calcula a acurácia\n",
    "        acc = accuracy_score(y_teste, y_pred)\n",
    "        print(f'\\nA Acurácia do Classificador {k} é {acc}')\n",
    "        \n",
    "        # Cria a Confusion Matrix\n",
    "        cm = confusion_matrix(y_teste, y_pred)\n",
    "        print(f'\\nConfusion Matrix')\n",
    "        print(cm)\n",
    "        \n",
    "        # Salva o modelo em disco\n",
    "        dump(clf, 'modelos/modelo_' + k + '_genero.joblib') \n",
    "        print(\"\\nModelo \" + k + \" salvo em disco.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa a função\n",
    "treina_avalia_genero(classificadores, X_treino, X_teste, y_treino, y_teste)   \n",
    "print(\"\\nTreinamento Concluído.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fKwQumi_nSCc"
   },
   "source": [
    "Como podemos ver acima, o classificador Random Forest fez um trabalho melhor na previsão de gênero do que o Xgboost e a árvore de decisão sem ajuste de hiperparâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 2 - Prever gênero e emoção do ator no áudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar o modelo para prever a emoção e o gênero do ator no áudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando o valor de x (sem os labels)\n",
    "X1 = df_final[df_final.columns[:-7]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando Label Encoder para converter a string em representação numérica\n",
    "y1 = LabelEncoder().fit_transform(df_final['LABEL_GENERO_EMOCAO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão em dados de treino e de teste\n",
    "X1_treino, X1_teste, y1_treino, y1_teste = train_test_split(X1, y1, test_size = 0.33, stratify = y, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "X1_treino.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "y1_treino.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "X1_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "y1_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos o scaler nos dados de treino\n",
    "fitted = z_scaler.fit(X1_treino)\n",
    "\n",
    "# E transformamos dados de treino e de teste\n",
    "X1_treino = fitted.transform(X1_treino)\n",
    "X1_teste = fitted.transform(X1_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para treinar cada classificador de gênero e emoção e avaliar a performance \n",
    "def treina_avalia_genero_emocao(classificadores, X_treino, X_teste, y_treino, y_teste):\n",
    "    \n",
    "    # Loop pelo dicionário de classificadores\n",
    "    for k, clf in classificadores.items():\n",
    "        \n",
    "        print(\"\\nIniciando o Treinamento do Modelo \" + k + \"...\")\n",
    "        \n",
    "        # Treina o modelo\n",
    "        clf.fit(X_treino, y_treino)\n",
    "        \n",
    "        # Faz a previsão\n",
    "        y_pred = clf.predict(X_teste)\n",
    "        \n",
    "        # Calcula a acurácia\n",
    "        acc = accuracy_score(y_teste, y_pred)\n",
    "        print(f'\\nA Acurácia do Classificador {k} é {acc}')\n",
    "        \n",
    "        # Cria a Confusion Matrix\n",
    "        cm = confusion_matrix(y_teste, y_pred)\n",
    "        print(f'\\nConfusion Matrix')\n",
    "        print(cm)\n",
    "        \n",
    "        # Salva o modelo em disco\n",
    "        dump(clf, 'modelos/modelo_' + k + '_genero_emocao.joblib') \n",
    "        print(\"\\nModelo \" + k + \" salvo em disco.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "X8naP9P8qp7i",
    "outputId": "058acfdb-5e8d-4bf0-9d88-d1cce3d31f13"
   },
   "outputs": [],
   "source": [
    "# Executa a função\n",
    "treina_avalia_genero_emocao(classificadores, X1_treino, X1_teste, y1_treino, y1_teste) \n",
    "print(\"\\nTreinamento Concluído.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pOFzKGF1LM91"
   },
   "source": [
    "Como vemos, o desempenho é melhor com RandomForest para prever a emoção e gênero do ator no áudio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 3 - Prever gênero do ator no áudio usando componentes PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0VMmaIUsUl8"
   },
   "source": [
    "Vamos verificar se aplicando a redução de dimensionalidade aos dados conseguimos melhorar a performance do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "yN_fAwo2bdWL",
    "outputId": "d3989dfb-8af1-4193-a675-c1ea9d17035f"
   },
   "outputs": [],
   "source": [
    "# Cria o objeto PCA com 20 componentes\n",
    "pca1 = PCA(n_components = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos X_treino criado para o Modelo 1\n",
    "X_treino.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica o PCA aos dados de treino \n",
    "pca_feature = pca1.fit(X_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize = (12, 4))\n",
    "plt.plot(pca_feature.explained_variance_ratio_, marker = 'o', label = 'Taxa de Variância Explicada')\n",
    "plt.plot(pca_feature.explained_variance_ratio_.cumsum(), marker = 'o', label = 'Taxa Acumulada de Variância Explicada')\n",
    "plt.legend()\n",
    "plt.ylabel('Variância')\n",
    "plt.xlabel('Número de Componentes Principais')\n",
    "plt.title('Variância vs Número de Componentes Principais')\n",
    "plt.xticks(np.arange(0, 20, step = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KcsEXGgSNtOs"
   },
   "source": [
    "Como vemos no gráfico (linha laranja), 15 componentes explicam mais de 95% da variação nos dados. Vamos então usar 15 componentes ao invés de 20 variáveis explicativas para treinar o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos gravar os componentes\n",
    "componentes = pca_feature.fit_transform(X_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza os dados\n",
    "componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confere o tipo\n",
    "type(componentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos conveter o array em dataframe\n",
    "componentes_df = pd.DataFrame(componentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confere o tipo\n",
    "type(componentes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos ajustar o índice\n",
    "componentes_df.index = pd.RangeIndex(start = 0, stop = len(componentes_df.index), step = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print do índice\n",
    "print(componentes_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza os dados\n",
    "componentes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos extrair 15 componentes e treinar o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FV52XCeGOJB9"
   },
   "outputs": [],
   "source": [
    "# Extraindo 15 componentes para x\n",
    "Xpca_treino = componentes_df[componentes_df.columns[0:15]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo y\n",
    "ypca_treino = y_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para os dados de teste, usamos o que havia sido definido para o Modelo 1\n",
    "Xpca_teste = X_teste\n",
    "ypca_teste = y_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Xpca_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ypca_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Xpca_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ypca_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para treinar cada classificador de gênero e avaliar a performance usando componentes PCA\n",
    "def treina_avalia_genero_pca(classificadores, X_treino, X_teste, y_treino, y_teste):\n",
    "    \n",
    "    # Loop pelo dicionário de classificadores\n",
    "    for k, clf in classificadores.items():\n",
    "        \n",
    "        print(\"\\nIniciando o Treinamento do Modelo \" + k + \"...\")\n",
    "        \n",
    "        # Treina o modelo\n",
    "        clf.fit(X_treino, y_treino)\n",
    "        \n",
    "        # Faz a previsão\n",
    "        y_pred = clf.predict(X_teste)\n",
    "        \n",
    "        # Calcula a acurácia\n",
    "        acc = accuracy_score(y_teste, y_pred)\n",
    "        print(f'\\nA Acurácia do Classificador {k} é {acc}')\n",
    "        \n",
    "        # Cria a Confusion Matrix\n",
    "        cm = confusion_matrix(y_teste, y_pred)\n",
    "        print(f'\\nConfusion Matrix')\n",
    "        print(cm)\n",
    "        \n",
    "        # Salva o modelo em disco\n",
    "        dump(clf, 'modelos/modelo_' + k + '_genero_pca.joblib') \n",
    "        print(\"\\nModelo \" + k + \" salvo em disco.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento e avaliação do modelo\n",
    "treina_avalia_genero_pca(classificadores, Xpca_treino, Xpca_teste, ypca_treino, ypca_teste)\n",
    "print(\"\\nTreinamento Concluído.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O6z09Q8cP4Fk"
   },
   "source": [
    "Vemos que o desempenho diminui reduzindo o número de recursos para prever o gênero. Vamos ficar com a primeira versão do modelo e na Parte 2 tentamos melhorar o desempenho com Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Data Wrangling.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
