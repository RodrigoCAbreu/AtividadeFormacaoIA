{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy - Programação Paralela em GPU</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nvidia CUDA em GPUs - Revisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos checar as GPUs do servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul  2 18:05:46 2017       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  TITAN X (Pascal)    Off  | 0000:05:00.0      On |                  N/A |\r\n",
      "| 23%   40C    P8    16W / 250W |    271MiB / 12188MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce GTX 108...  Off  | 0000:09:00.0     Off |                  N/A |\r\n",
      "| 23%   29C    P8     9W / 250W |      1MiB / 11172MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1398    G   /usr/lib/xorg/Xorg                             174MiB |\r\n",
      "|    0      2795    G   compiz                                          94MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paralelismo\n",
    "\n",
    "Para a primeira tarefa, vamos usar os seguintes conceitos:\n",
    "\n",
    "* <code style=\"color:green\">&#95;&#95;global&#95;&#95;</code> - Esta palavra-chave é um qualificador usado para dizer ao compilador CUDA que a função deve ser compilada para a GPU. Para o CUDA C/C ++, o compilador nvcc irá lidar com a compilação deste código.\n",
    "* <code style=\"color:green\">blockIdx.x</code> - Esta é uma variável usada dentro de um kernel de GPU para determinar a ID do bloco que está atualmente executando o código. Uma vez que haverá muitos blocos em paralelo, precisamos desta ID para ajudar a determinar qual parte dos dados um bloco particular funcionará.\n",
    "* <code style=\"color:green\">threadIdx.x</code> - Esta é uma variável usada dentro de um kernel de GPU para determinar o ID da thread que está atualmente executando o código no bloco ativo.\n",
    "* <code style=\"color:green\">blockDim.x</code> - Esta é uma variável que retorna um valor que indica o número de threads que há por bloco. Lembre-se de que todos os blocos agendados para executar na GPU são idênticos, exceto para o valor de <code style=\"color:green\">blockIdx.x</code>.\n",
    "* <code style=\"color:green\">myKernel <<< numero_de_blocos, threads_por_bloco>>> (...)</code> -  Esta é a sintaxe usada para iniciar um kernel na GPU. Dentro de \"<<< >>>\", estabelecemos dois valores. O primeiro é o número total de blocos que queremos executar na GPU, e o segundo é o número de threads que há por bloco. \n",
    "\n",
    "Vamos explorar os conceitos acima, fazendo um simples exemplo de \"Hello Paralelismo\". Ao executar a célula abaixo, teremos:\n",
    "\n",
    "1. A partir do arquivo de origem .cu, código separado que deve ser compilado para a GPU e o código que deve ser compilado para a CPU\n",
    "2. O nvcc compilará o próprio código GPU\n",
    "3. nvcc dará ao compilador do host, no nosso caso gcc, o código da CPU para compilar\n",
    "4. Vincula o código compilado de # 2 e # 3 e crie o executável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <stdio.h>\r\n",
      "\r\n",
      "#define NUM_BLOCKS 16\r\n",
      "#define BLOCK_WIDTH 1\r\n",
      "\r\n",
      "__global__ void hello()\r\n",
      "{\r\n",
      "    printf(\"Olá! Eu sou uma thread no bloco %d\\n\", blockIdx.x);\r\n",
      "}\r\n",
      "\r\n",
      "\r\n",
      "int main(int argc,char **argv)\r\n",
      "{\r\n",
      "    // Inicializa o kernel\r\n",
      "    hello<<<NUM_BLOCKS, BLOCK_WIDTH>>>();\r\n",
      "\r\n",
      "    // Sincroniza todas as threads antes de passar o controle de volta para a CPU\r\n",
      "    cudaDeviceSynchronize();\r\n",
      "\r\n",
      "    printf(\"Processamento Concluído!\\n\");\r\n",
      "\r\n",
      "    return 0;\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat exemplo1/exemplo1.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá! Eu sou uma thread no bloco 15\r\n",
      "Olá! Eu sou uma thread no bloco 8\r\n",
      "Olá! Eu sou uma thread no bloco 14\r\n",
      "Olá! Eu sou uma thread no bloco 9\r\n",
      "Olá! Eu sou uma thread no bloco 6\r\n",
      "Olá! Eu sou uma thread no bloco 4\r\n",
      "Olá! Eu sou uma thread no bloco 5\r\n",
      "Olá! Eu sou uma thread no bloco 3\r\n",
      "Olá! Eu sou uma thread no bloco 12\r\n",
      "Olá! Eu sou uma thread no bloco 11\r\n",
      "Olá! Eu sou uma thread no bloco 0\r\n",
      "Olá! Eu sou uma thread no bloco 13\r\n",
      "Olá! Eu sou uma thread no bloco 1\r\n",
      "Olá! Eu sou uma thread no bloco 2\r\n",
      "Olá! Eu sou uma thread no bloco 7\r\n",
      "Olá! Eu sou uma thread no bloco 10\r\n",
      "Processamento Concluído!\r\n"
     ]
    }
   ],
   "source": [
    "# Compila o exemplo1 e executa o programa gerado\n",
    "!nvcc -arch=sm_30 -o exemplo1/exemplo1_out exemplo1/exemplo1.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializando um kernel na GPU - Unified Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <stdio.h>\r\n",
      "#include <iostream>\r\n",
      "\r\n",
      "// Número de elementos em cada vetor\r\n",
      "#define N 2048 * 2048 \r\n",
      "\r\n",
      "__global__ void my_kernel(int * a, int * b, int * c)\r\n",
      "{\r\n",
      "    // Determina a identificação de thread global exclusiva, por isso sabemos qual elemento processar\r\n",
      "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\r\n",
      "    \r\n",
      "    if ( tid < N ) // Certifique-se de que não inicializamos mais threads do que o necessário\r\n",
      "        c[tid] = a[tid] + b[tid];\r\n",
      "}\r\n",
      "\r\n",
      "void report_gpu_mem()\r\n",
      "{\r\n",
      "    size_t free, total;\r\n",
      "    cudaMemGetInfo(&free, &total);\r\n",
      "    std::cout << \"Free = \" << free << \" Total = \" << total <<std::endl;\r\n",
      "}\r\n",
      "\r\n",
      "int main()\r\n",
      "{\r\n",
      "    int *a, *b, *c;\r\n",
      "\r\n",
      "    // Número total de bytes por vetor\r\n",
      "    int size = N * sizeof (int); \r\n",
      "\r\n",
      "    // Aloca memória sem a necessidade de usar cudaMemcpy\r\n",
      "    cudaMallocManaged(&a, size);\r\n",
      "    cudaMallocManaged(&b, size);\r\n",
      "    cudaMallocManaged(&c, size);\r\n",
      "\r\n",
      "    // Inicializa memória\r\n",
      "    for( int i = 0; i < N; ++i )\r\n",
      "    {\r\n",
      "        a[i] = i;\r\n",
      "        b[i] = i;\r\n",
      "        c[i] = 0;\r\n",
      "    }\r\n",
      "\r\n",
      "    int threads_per_block = 128;\r\n",
      "    int number_of_blocks = (N / threads_per_block) + 1;\r\n",
      "\r\n",
      "    my_kernel <<< number_of_blocks, threads_per_block >>> ( a, b, c );\r\n",
      "\r\n",
      "    // Espera até a GPU finalizar\r\n",
      "    cudaDeviceSynchronize(); \r\n",
      "\r\n",
      "    // Imprime os últimos 5 valores de c \r\n",
      "    for( int i = N-5; i < N; ++i )\r\n",
      "        printf(\"c[%d] = %d, \", i, c[i]);\r\n",
      "    printf (\"\\n\");\r\n",
      "\r\n",
      "    // Libera toda a nossa memória alocada\r\n",
      "    report_gpu_mem();\r\n",
      "    cudaFree( a );\r\n",
      "    report_gpu_mem(); \r\n",
      "    cudaFree( b );\r\n",
      "    report_gpu_mem(); \r\n",
      "    cudaFree( c );\r\n",
      "    report_gpu_mem();\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat exemplo2/exemplo2.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[4194299] = 8388598, c[4194300] = 8388600, c[4194301] = 8388602, c[4194302] = 8388604, c[4194303] = 8388606, \r\n",
      "Free = 11504386048 Total = 11715084288\r\n",
      "Free = 11521163264 Total = 11715084288\r\n",
      "Free = 11537940480 Total = 11715084288\r\n",
      "Free = 11554717696 Total = 11715084288\r\n"
     ]
    }
   ],
   "source": [
    "# Compila o exemplo2 e executa o programa gerado\n",
    "!nvcc -arch=sm_30 -o exemplo2/exemplo2_out exemplo2/exemplo2.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acelerando Operações com Matrizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <stdio.h>\r\n",
      "#include \"cuda_runtime.h\"\r\n",
      "#include \"device_launch_parameters.h\"\r\n",
      "#include <iostream>\r\n",
      "#include <time.h>\r\n",
      "using namespace std;\r\n",
      "\r\n",
      "#define N 756\r\n",
      "\r\n",
      "// kernel\r\n",
      "__global__ void matrixMulGPU( int * a, int * b, int * c )\r\n",
      "{\r\n",
      "    int val = 0;\r\n",
      "\r\n",
      "    int row = blockIdx.x * blockDim.x + threadIdx.x;\r\n",
      "    int col = blockIdx.y * blockDim.y + threadIdx.y;\r\n",
      "\r\n",
      "    if (row < N && col < N)\r\n",
      "    {\r\n",
      "        for ( int k = 0; k < N; ++k )\r\n",
      "            val += a[row * N + k] * b[k * N + col];\r\n",
      "        c[row * N + col] = val;\r\n",
      "    }\r\n",
      "}\r\n",
      "\r\n",
      "void matrixMulCPU( int * a, int * b, int * c )\r\n",
      "{\r\n",
      "    int val = 0;\r\n",
      "\r\n",
      "    for( int row = 0; row < N; ++row )\r\n",
      "        for( int col = 0; col < N; ++col )\r\n",
      "        {\r\n",
      "            val = 0;\r\n",
      "            for ( int k = 0; k < N; ++k )\r\n",
      "                val += a[row * N + k] * b[k * N + col];\r\n",
      "            c[row * N + col] = val;\r\n",
      "        }\r\n",
      "}\r\n",
      "\r\n",
      "int main()\r\n",
      "{\r\n",
      "    int *a, *b, *c_cpu, *c_gpu;\r\n",
      "\r\n",
      "    // Número de bytes de uma matriz N x N \r\n",
      "    int size = N * N * sizeof (int); \r\n",
      "\r\n",
      "    // Aloca memória\r\n",
      "    cudaMallocManaged (&a, size);\r\n",
      "    cudaMallocManaged (&b, size);\r\n",
      "    cudaMallocManaged (&c_cpu, size);\r\n",
      "    cudaMallocManaged (&c_gpu, size);\r\n",
      "\r\n",
      "    // Inicializa memória\r\n",
      "    for( int row = 0; row < N; ++row )\r\n",
      "        for( int col = 0; col < N; ++col )\r\n",
      "        {\r\n",
      "            a[row * N + col] = row;\r\n",
      "            b[row * N + col] = col+2;\r\n",
      "            c_cpu[row * N + col] = 0;\r\n",
      "            c_gpu[row * N + col] = 0;\r\n",
      "        }\r\n",
      "\r\n",
      "    // Bloco de threads 16 x 16     \r\n",
      "    dim3 threads_per_block (16, 16, 1); \r\n",
      "    dim3 number_of_blocks ((N / threads_per_block.x) + 1, (N / threads_per_block.y) + 1, 1);\r\n",
      "\r\n",
      "    // Define 2 eventos CUDA\r\n",
      "    cudaEvent_t start, end;\r\n",
      "\r\n",
      "    // Cria os eventos\r\n",
      "    cudaEventCreate(&start);\r\n",
      "    cudaEventCreate(&end);\r\n",
      "\r\n",
      "    // Registra o primeiro evento\r\n",
      "    cudaEventRecord(start);\r\n",
      "\r\n",
      "    // Chamada ao kernel\r\n",
      "    matrixMulGPU <<< number_of_blocks, threads_per_block >>> ( a, b, c_gpu );\r\n",
      "\r\n",
      "    // Registra o segundo evento\r\n",
      "    cudaEventRecord(end);\r\n",
      "\r\n",
      "    // Aguarda a GPU finalizar seu trabalho\r\n",
      "    cudaDeviceSynchronize(); \r\n",
      "\r\n",
      "    // Calcula o tempo usado no processamento\r\n",
      "    float elapsed;\r\n",
      "    cudaEventElapsedTime(&elapsed, start, end);\r\n",
      "\r\n",
      "    cout << \"Tempo de processamento na GPU igual a \" << elapsed << \" msec (aproximadamente 0.01108 segundos)\" << endl;\r\n",
      "\r\n",
      "    clock_t start1, end1;\r\n",
      "    double cpu_time_used;\r\n",
      "\r\n",
      "    start1 = clock();\r\n",
      "\r\n",
      "    // Chama a versão para CPU para checar nosso trabalho\r\n",
      "    matrixMulCPU( a, b, c_cpu );\r\n",
      "\r\n",
      "    // Calcula o tempo usado no processamento\r\n",
      "    end1 = clock();\r\n",
      "    cpu_time_used = ((double) (end1 - start1)) / CLOCKS_PER_SEC;\r\n",
      "\r\n",
      "    cout << \"Tempo de processamento na CPU igual a \" << cpu_time_used << \" sec\" << endl;\r\n",
      "\r\n",
      "    // Compara as duas respostas para garantir que elas sejam iguais\r\n",
      "    bool error = false;\r\n",
      "    for( int row = 0; row < N && !error; ++row )\r\n",
      "        for( int col = 0; col < N && !error; ++col )\r\n",
      "            if (c_cpu[row * N + col] != c_gpu[row * N + col])\r\n",
      "            {\r\n",
      "                printf(\"FOUND ERROR at c[%d][%d]\\n\", row, col);\r\n",
      "                error = true;\r\n",
      "                break;\r\n",
      "            }\r\n",
      "    if (!error)\r\n",
      "        printf(\"Successo! As duas matrizes são iguais, sendo executadas na CPU e na GPU!\\n\");\r\n",
      "\r\n",
      "    // Libera a memória\r\n",
      "    cudaFree(a); \r\n",
      "    cudaFree(b);\r\n",
      "    cudaFree( c_cpu ); \r\n",
      "    cudaFree( c_gpu );\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat exemplo3/exemplo3.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento na GPU igual a 10.4144 msec (aproximadamente 0.01108 segundos)\n",
      "Tempo de processamento na CPU igual a 1.36594 sec\n",
      "Successo! As duas matrizes são iguais, sendo executadas na CPU e na GPU!\n"
     ]
    }
   ],
   "source": [
    "# Compila o exemplo3 e executa o programa gerado\n",
    "!nvcc -arch=sm_30 -o exemplo3/exemplo3_out exemplo3/exemplo3.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento de Erro\n",
    "\n",
    "Se você alterar consideravelmente o número de blocos e threads por bloco nos exemplos acima, você pode notar alguns casos em que você não receba a resposta esperada. Até este ponto, não adicionamos nenhum tipo de verificação de erros, o que torna muito difícil dizer por que um problema está ocorrendo. A verificação de erros é tão importante quando a programação para um GPU quanto para uma CPU. Então vamos adicionar uma verificação de erro e ver se podemos introduzir alguns erros para capturar.\n",
    "\n",
    "**Nota**: É altamente encorajado que você inclua verificação de erros em seu código sempre que possível!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <stdio.h>\r\n",
      "\r\n",
      "// Número de elementos em cada vetor\r\n",
      "#define N 2048 * 2048\r\n",
      "\r\n",
      "__global__ void my_kernel(float scalar, float * x, float * y)\r\n",
      "{\r\n",
      "    // Determina a identificação de thread global exclusiva, por isso sabemos qual elemento processar\r\n",
      "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\r\n",
      "    \r\n",
      "    // Certifique-se de que ainda temos threads disponíveis!\r\n",
      "    if ( tid < N ) \r\n",
      "        y[tid] = scalar * x[tid] + y[tid];\r\n",
      "}\r\n",
      "\r\n",
      "int main()\r\n",
      "{\r\n",
      "    float *x, *y;\r\n",
      "\r\n",
      "    // O número total de bytes por vetor\r\n",
      "    int size = N * sizeof (float); \r\n",
      "\r\n",
      "    cudaError_t ierrAsync;\r\n",
      "    cudaError_t ierrSync;\r\n",
      "\r\n",
      "    // Aloca memória\r\n",
      "    cudaMallocManaged(&x, size);\r\n",
      "    cudaMallocManaged(&y, size);\r\n",
      "\r\n",
      "    // Inicializa a memória\r\n",
      "    for( int i = 0; i < N; ++i )\r\n",
      "    {\r\n",
      "        x[i] = 1.0f;\r\n",
      "        y[i] = 2.0f;\r\n",
      "    }\r\n",
      "\r\n",
      "    int threads_per_block = 256;\r\n",
      "    int number_of_blocks = (N / threads_per_block) + 1;\r\n",
      "\r\n",
      "    my_kernel <<< number_of_blocks, threads_per_block >>> ( 2.0f, x, y );\r\n",
      "\r\n",
      "    ierrSync = cudaGetLastError();\r\n",
      "\r\n",
      "    // Aguarde até que a GPU termine\r\n",
      "    ierrAsync = cudaDeviceSynchronize(); \r\n",
      "\r\n",
      "    // Verifica status de execução\r\n",
      "    if (ierrSync != cudaSuccess) { printf(\"Sync error: %s\\n\", cudaGetErrorString(ierrSync)); }\r\n",
      "    if (ierrAsync != cudaSuccess) { printf(\"Async error: %s\\n\", cudaGetErrorString(ierrAsync)); }\r\n",
      "\r\n",
      "    // Imprime o erro máximo\r\n",
      "    float maxError = 0;\r\n",
      "    for( int i = 0; i < N; ++i )\r\n",
      "        if (abs(4-y[i]) > maxError) { maxError = abs(4-y[i]); }\r\n",
      "    printf(\"Max Error: %.5f\", maxError);\r\n",
      "\r\n",
      "    // Libera a memória alocada\r\n",
      "    cudaFree( x ); cudaFree( y );\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat exemplo4/exemplo4.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Error: 0.00000"
     ]
    }
   ],
   "source": [
    "# Compila o exemplo4 e executa o programa gerado\n",
    "!nvcc -arch=sm_30 -o exemplo4/exemplo4_out exemplo4/exemplo4.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultado os Parâmetros da GPU\n",
    "\n",
    "A API de gerenciamento de dispositivos CUDA C / C ++ permite que um programador consulte o número de dispositivos disponíveis em um sistema e os recursos de cada dispositivo. O código simples abaixo ilustra o uso da API de gerenciamento de dispositivos. Depois que o número de dispositivos habilitados para CUDA conectados ao sistema é determinado via `cudaGetDeviceCount()`, um loop sobre esses dispositivos é realizado (observe que os dispositivos são enumerados a partir de 0) e a função `cudaGetDeviceProperties()` é usada para retornar informações sobre um dispositivo em uma variável de tipo `cudaDeviceProp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <stdio.h>\r\n",
      "\r\n",
      "#define NX 200\r\n",
      "#define NY 100\r\n",
      "\r\n",
      "__global__ void my_kernel2D(float scalar, float * x, float * y)\r\n",
      "{\r\n",
      "    int row = blockIdx.x * blockDim.x + threadIdx.x;\r\n",
      "    int col = blockIdx.y * blockDim.y + threadIdx.y;\r\n",
      "    \r\n",
      "    // Verifica se ainda temos threads antes de executar a operação\r\n",
      "    if ( row < NX && col < NY ) \r\n",
      "        y[row * NY + col] = scalar * x[row * NY + col] + y[row * NY + col];\r\n",
      "}\r\n",
      "\r\n",
      "int main()\r\n",
      "{\r\n",
      "    float *x, *y;\r\n",
      "    float maxError = 0;\r\n",
      "\r\n",
      "    // Total de bytes por vetor\r\n",
      "    int size = NX * NY * sizeof (float); \r\n",
      "\r\n",
      "    cudaError_t ierrAsync;\r\n",
      "    cudaError_t ierrSync;\r\n",
      "\r\n",
      "    cudaDeviceProp prop;\r\n",
      "\r\n",
      "    // Aloca memória\r\n",
      "    cudaMallocManaged(&x, size);\r\n",
      "    cudaMallocManaged(&y, size);\r\n",
      "\r\n",
      "    // Inicializa memória\r\n",
      "    for( int i = 0; i < NX*NY; ++i )\r\n",
      "    {\r\n",
      "        x[i] = 1.0f;\r\n",
      "        y[i] = 2.0f;\r\n",
      "    }\r\n",
      "\r\n",
      "    dim3 threads_per_block (32,16,1);\r\n",
      "    dim3 number_of_blocks ((NX/threads_per_block.x)+1, (NY/threads_per_block.y)+1, 1);\r\n",
      "\r\n",
      "    cudaGetDeviceProperties(&prop, 0);\r\n",
      "    if (threads_per_block.x * threads_per_block.y * threads_per_block.z > prop.maxThreadsPerBlock) {\r\n",
      "        printf(\"Muitas threads por bloco ... finalizando\\n\");\r\n",
      "        goto cleanup;\r\n",
      "    }\r\n",
      "    if (threads_per_block.x > prop.maxThreadsDim[0]) {\r\n",
      "        printf(\"Muitas threads na dimensão x ... finalizando\\n\");\r\n",
      "        goto cleanup;\r\n",
      "    }\r\n",
      "    if (threads_per_block.y > prop.maxThreadsDim[1]) {\r\n",
      "        printf(\"Muitas threads na dimensão y ... finalizando\\n\");\r\n",
      "        goto cleanup;\r\n",
      "    }\r\n",
      "    if (threads_per_block.z > prop.maxThreadsDim[2]) {\r\n",
      "        printf(\"Muitas threads na dimensão z ... finalizando\\n\");\r\n",
      "        goto cleanup;\r\n",
      "    }\r\n",
      "\r\n",
      "    my_kernel2D <<< number_of_blocks, threads_per_block >>> ( 2.0f, x, y );\r\n",
      "\r\n",
      "    ierrSync = cudaGetLastError();\r\n",
      "\r\n",
      "    // Espera a GPU finalizar\r\n",
      "    ierrAsync = cudaDeviceSynchronize(); \r\n",
      "    if (ierrSync != cudaSuccess) { printf(\"Sync error: %s\\n\", cudaGetErrorString(ierrSync)); }\r\n",
      "    if (ierrAsync != cudaSuccess) { printf(\"Async error: %s\\n\", cudaGetErrorString(ierrAsync)); }\r\n",
      "\r\n",
      "    // Imprime o erro\r\n",
      "    for( int i = 0; i < NX*NY; ++i )\r\n",
      "        if (abs(4-y[i]) > maxError) { maxError = abs(4-y[i]); }\r\n",
      "    printf(\"Max Error: %.5f\", maxError);\r\n",
      "\r\n",
      "cleanup:\r\n",
      "    // Libera memória alocada\r\n",
      "    cudaFree( x ); cudaFree( y );\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat exemplo5/exemplo5.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tente digitar tamanhos diferentes na linha de dimensão do bloco, `dim3 threads_per_block (32,16,1);` e verifique se o seu novo controle de propriedade do dispositivo GPU funciona corretamente!\n",
    "\n",
    "À medida que você começa a escrever código de GPU que possivelmente poderia executar em múltiplos ou diferentes tipos de GPUs, você deve usar a capacidade de consultar facilmente cada dispositivo para determinar a configuração ideal para seu código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Error: 0.00000"
     ]
    }
   ],
   "source": [
    "# Compila o exemplo5 e executa o programa gerado\n",
    "!nvcc -arch=sm_30 -o exemplo5/exemplo5_out exemplo5/exemplo5.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerenciamento de Memória\n",
    "\n",
    "É importante perceber que a GPU tem sua própria memória física; Assim como a CPU usa a RAM do sistema para sua memória. Ao executar o código na GPU, temos de garantir que todos os dados necessários sejam copiados primeiro no barramento PCI-Express para a memória da GPU antes de iniciar nossos kernels.\n",
    "\n",
    "* `cudaMalloc ( void** devPtr, size_t size )` - Esta chamada de API é usada para alocar memória na GPU, e é muito semelhante ao uso de `malloc` na CPU. Você fornece o endereço de um ponteiro que apontará para a memória após a conclusão da chamada, assim como o número de bytes a serem alocados.\n",
    "\n",
    "* `cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind )` - Também é muito semelhante ao padrão `memcpy`, esta chamada API é usada para copiar dados entre a CPU e o GPU. É preciso um ponteiro de destino, um ponteiro de origem, o número de bytes a copiar e o quarto parâmetro indica qual direção os dados estão viajando: GPU-> CPU, CPU-> GPU ou GPU-> GPU.\n",
    "\n",
    "* `cudaFree ( void* devPtr )` - Usamos essa chamada de API para liberar qualquer memória que alocamos no GPU.\n",
    "\n",
    "* `cudaMallocManaged ( T** devPtr, size_t size );` - aloca `size` bytes na memória gerenciada e armazena em devPtr.\n",
    "\n",
    "* `cudaFree ( void* devPtr )` - Usamos essa chamada de API para liberar qualquer memória que alocamos na memória gerenciada.\n",
    "\n",
    "Depois de ter usado `cudaMallocManaged` para alocar alguns dados, você apenas usa o ponteiro em seu código, independentemente de ser a CPU ou a GPU acessando os dados. Antes da Memória Unificada, normalmente você tinha dois indicadores associados aos dados; Um para a memória da CPU e um para a memória do GPU (geralmente usando o nome da GPU precedido com um `d_` para indicar a memória do dispositivo).\n",
    "\n",
    "A memória gerenciada é sincronizada entre os espaços de memória no lançamento do kernel e quaisquer pontos de sincronização do dispositivo. Isso significa que, nas arquiteturas Kepler e Maxwell, um ponto de sincronização explícito (normalmente `cudaDeviceSynchronize ()`) precisa ser inserido após um lançamento do kernel, mas antes que o host use dados gerados por esse kernel. Visite a documentação CUDA [page on Unified Memory](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-unified-memory-programming-hd) para mais detalhes sobre memória unificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <string.h>\r\n",
      "#include <stdio.h>\r\n",
      "\r\n",
      "struct DataElement\r\n",
      "{\r\n",
      "  char *name;\r\n",
      "  int value;\r\n",
      "};\r\n",
      "\r\n",
      "__global__ void Kernel(DataElement *elem) {\r\n",
      "  printf(\"On device: name=%s, value=%d\\n\", elem->name, elem->value);\r\n",
      "\r\n",
      "  elem -> name[0] = 'd';\r\n",
      "  elem -> value++;\r\n",
      "}\r\n",
      "\r\n",
      "void launch(DataElement *elem) {\r\n",
      "  Kernel <<< 1, 1 >>> (elem);\r\n",
      "  cudaDeviceSynchronize();\r\n",
      "}\r\n",
      "\r\n",
      "int main(void)\r\n",
      "{\r\n",
      "  DataElement *e;\r\n",
      "  cudaMallocManaged((void**)&e, sizeof(DataElement));\r\n",
      "\r\n",
      "  e->value = 10;\r\n",
      "  cudaMallocManaged((void**)&(e->name), sizeof(char) * (strlen(\"hello\") + 1) );\r\n",
      "  strcpy(e->name, \"hello\");\r\n",
      "\r\n",
      "  launch(e);\r\n",
      "\r\n",
      "  printf(\"On host: name=%s, value=%d\\n\", e->name, e->value);\r\n",
      "\r\n",
      "  cudaFree(e->name);\r\n",
      "  cudaFree(e);\r\n",
      "\r\n",
      "  cudaDeviceReset();\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat exemplo6/exemplo6.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode ver por que a memória unificada é atraente - ela remove o requisito de código de gerenciamento de dados complexo. Permitindo que você obtenha suas funções executando na GPU com menos esforço de desenvolvimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device: name=hello, value=10\r\n",
      "On host: name=dello, value=11\r\n"
     ]
    }
   ],
   "source": [
    "# Compila o exemplo6 e executa o programa gerado\n",
    "!nvcc -arch=sm_30 -o exemplo6/exemplo6_out exemplo6/exemplo6.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposta da Matriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo vamos programar um algoritmo para [Transposta da Matriz](http://en.wikipedia.org/wiki/Transpose).  Por motivos de simplicidade, usaremos matrizes quadradas. Isso nos permitirá focar as importantes técnicas de otimização de memória sem se preocupar com matrizes de forma desigual. \n",
    "\n",
    "O algoritmo de transposição da matriz é definido como $A_{i,j} = B_{j,i}$ onde $A$ e $B$ são $M \\times M$ matrizes e os índices $i,j$ são os índices de linha e coluna, respectivamente.  (Nos exercícios de hoje vamos usar [column-major](http://en.wikipedia.org/wiki/Row-major_order#Column-major_order) para ordenação dos elementos.)\n",
    "\n",
    "Por exemplo, se você tem um $3 \\times 3$ matriz $A$ como a seguinte $$A = \\left( \\begin{array}{ccc}\n",
    "a & d & g \\\\\n",
    "b & e & h \\\\\n",
    "c & f & i \\end{array} \\right),$$\n",
    "então a transposta da matriz dado por $A^{T}$ é\n",
    "$$A^{T} = \\left( \\begin{array}{ccc}\n",
    "a & b & c \\\\\n",
    "d & e & f \\\\\n",
    "g & h & i \\end{array} \\right).$$\n",
    "\n",
    "Este exemplo consiste em três tarefas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Checking\n",
    "\n",
    "Uma das técnicas de programação mais importantes para escrever código robusto é fazer uma verificação de erros adequada. Todas as funções de tempo de execução em CUDA retornam um código de erro do tipo ** `cudaError_t` **. É uma boa prática verificar o código de erro retornado de todas as funções CUDA. Neste exemplo 7, fornecemos duas macros para ajudá-lo a fazer isso. Primeiro, você pode usar `CUDA_CALL (F)` para envolver cada chamada que você faz na API de tempo de execução do CUDA. Por exemplo, em vez de escrever\n",
    "\n",
    "```cpp\n",
    "cudaMemcpy( h_c, c, sizeof(float), cudaMemcpyHostToDevice );\n",
    "```\n",
    "\n",
    "você poderia escrever\n",
    "\n",
    "```cpp\n",
    "CUDA_CALL( cudaMemcpy( h_c, c, sizeof(float), cudaMemcpyHostToDevice ) );\n",
    "```\n",
    "\n",
    "e isso irá verificar o código de retorno do `cudaMemcpy` e informá-lo se houver um erro.\n",
    "\n",
    "Existe uma exceção para esse uso e é quando se chama kernels. Kernels não retornam nenhum valor. Para verificar se um kernel foi iniciado corretamente, você pode fazer o seguinte. Se você tiver um lançamento do kernel\n",
    "\n",
    "```cpp\n",
    "kernel<<< 256, 256 >>>( d_a, d_b, d_c );\n",
    "```\n",
    "\n",
    "você usaria a macro `CUDA_CHECK()` seguida por `CUDA_CALL( cudaDeviceSynchronize )` conforme abaixo\n",
    "\n",
    "```cpp\n",
    "kernel<<< 256, 256 >>>( d_a, d_b, d_c );\n",
    "CUDA_CHECK()\n",
    "CUDA_CALL( cudaDeviceSynchronize() );`\n",
    "```\n",
    "\n",
    "Nas macros de verificação de erros que fornecemos, se houver um erro, você receberá uma mensagem impressa na tela e o programa terminará. Se nenhum erro for detectado, o programa executará normalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*\r\n",
      " *  Copyright 2014 NVIDIA Corporation\r\n",
      " *\r\n",
      " *  Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
      " *  you may not use this file except in compliance with the License.\r\n",
      " *  You may obtain a copy of the License at\r\n",
      " *\r\n",
      " *      http://www.apache.org/licenses/LICENSE-2.0\r\n",
      " *\r\n",
      " *  Unless required by applicable law or agreed to in writing, software\r\n",
      " *  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
      " *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
      " *  See the License for the specific language governing permissions and\r\n",
      " *  limitations under the License.\r\n",
      " */\r\n",
      "\r\n",
      "#include <stdio.h>\r\n",
      "\r\n",
      "#ifdef DEBUG\r\n",
      "#define CUDA_CALL(F)  if( (F) != cudaSuccess ) \\\r\n",
      "  {printf(\"Error %s at %s:%d\\n\", cudaGetErrorString(cudaGetLastError()), \\\r\n",
      "   __FILE__,__LINE__); exit(-1);} \r\n",
      "#define CUDA_CHECK()  if( (cudaPeekAtLastError()) != cudaSuccess ) \\\r\n",
      "  {printf(\"Error %s at %s:%d\\n\", cudaGetErrorString(cudaGetLastError()), \\\r\n",
      "   __FILE__,__LINE__-1); exit(-1);} \r\n",
      "#else\r\n",
      "#define CUDA_CALL(F) (F)\r\n",
      "#define CUDA_CHECK() \r\n",
      "#endif\r\n",
      "\r\n",
      "/* definitions of threadblock size in X and Y directions */\r\n",
      "\r\n",
      "#define THREADS_PER_BLOCK_X 32\r\n",
      "#define THREADS_PER_BLOCK_Y 32\r\n",
      "\r\n",
      "/* definition of matrix linear dimension */\r\n",
      "\r\n",
      "#define SIZE 4096\r\n",
      "\r\n",
      "/* macro to index a 1D memory array with 2D indices in column-major order */\r\n",
      "\r\n",
      "#define INDX( row, col, ld ) ( ( (col) * (ld) ) + (row) )\r\n",
      "\r\n",
      "/* CUDA kernel for naive matrix transpose */\r\n",
      "\r\n",
      "__global__ void naive_cuda_transpose( const int m, \r\n",
      "                                      const double * const a, \r\n",
      "                                      double * const c )\r\n",
      "{\r\n",
      "  const int myRow = blockDim.x * blockIdx.x + threadIdx.x;\r\n",
      "  const int myCol = blockDim.y * blockIdx.y + threadIdx.y;\r\n",
      "\r\n",
      "  if( myRow < m && myCol < m )\r\n",
      "  {\r\n",
      "    c[INDX( myRow, myCol, m )] = a[INDX( myCol, myRow, m )];\r\n",
      "  } /* end if */\r\n",
      "  return;\r\n",
      "\r\n",
      "} /* end naive_cuda_transpose */\r\n",
      "\r\n",
      "void host_transpose( const int m, const double * const a, double *c )\r\n",
      "{\r\n",
      "\t\r\n",
      "/* \r\n",
      " *  naive matrix transpose goes here.\r\n",
      " */\r\n",
      " \r\n",
      "  for( int j = 0; j < m; j++ )\r\n",
      "  {\r\n",
      "    for( int i = 0; i < m; i++ )\r\n",
      "      {\r\n",
      "        c[INDX(i,j,m)] = a[INDX(j,i,m)];\r\n",
      "      } /* end for i */\r\n",
      "  } /* end for j */\r\n",
      "\r\n",
      "} /* end host_dgemm */\r\n",
      "\r\n",
      "int main( int argc, char *argv[] )\r\n",
      "{\r\n",
      "\r\n",
      "  int size = SIZE;\r\n",
      "\r\n",
      "  fprintf(stdout, \"Matrix size is %d\\n\",size);\r\n",
      "\r\n",
      "/* declaring pointers for array */\r\n",
      "\r\n",
      "  double *h_a, *h_c;\r\n",
      "  double *d_a, *d_c;\r\n",
      " \r\n",
      "  size_t numbytes = (size_t) size * (size_t) size * sizeof( double );\r\n",
      "\r\n",
      "/* allocating host memory */\r\n",
      "\r\n",
      "  h_a = (double *) malloc( numbytes );\r\n",
      "  if( h_a == NULL )\r\n",
      "  {\r\n",
      "    fprintf(stderr,\"Error in host malloc h_a\\n\");\r\n",
      "    return 911;\r\n",
      "  }\r\n",
      "\r\n",
      "  h_c = (double *) malloc( numbytes );\r\n",
      "  if( h_c == NULL )\r\n",
      "  {\r\n",
      "    fprintf(stderr,\"Error in host malloc h_c\\n\");\r\n",
      "    return 911;\r\n",
      "  }\r\n",
      "\r\n",
      "/* allocating device memory */\r\n",
      "\r\n",
      "  CUDA_CALL( cudaMalloc( (void**) &d_a, numbytes ) );\r\n",
      "  CUDA_CALL( cudaMalloc( (void**) &d_c, numbytes ) );\r\n",
      "\r\n",
      "/* set result matrices to zero */\r\n",
      "\r\n",
      "  memset( h_c, 0, numbytes );\r\n",
      "  CUDA_CALL( cudaMemset( d_c, 0, numbytes ) );\r\n",
      "\r\n",
      "  fprintf( stdout, \"Total memory required per matrix is %lf MB\\n\", \r\n",
      "     (double) numbytes / 1000000.0 );\r\n",
      "\r\n",
      "/* initialize input matrix with random value */\r\n",
      "\r\n",
      "  for( int i = 0; i < size * size; i++ )\r\n",
      "  {\r\n",
      "    h_a[i] = double( rand() ) / ( double(RAND_MAX) + 1.0 );\r\n",
      "  }\r\n",
      "\r\n",
      "/* copy input matrix from host to device */\r\n",
      "\r\n",
      "  CUDA_CALL( cudaMemcpy( d_a, h_a, numbytes, cudaMemcpyHostToDevice ) );\r\n",
      "\r\n",
      "/* create and start timer */\r\n",
      "\r\n",
      "  cudaEvent_t start, stop;\r\n",
      "  CUDA_CALL( cudaEventCreate( &start ) );\r\n",
      "  CUDA_CALL( cudaEventCreate( &stop ) );\r\n",
      "  CUDA_CALL( cudaEventRecord( start, 0 ) );\r\n",
      "\r\n",
      "/* call naive cpu transpose function */\r\n",
      "\r\n",
      "  host_transpose( size, h_a, h_c );\r\n",
      "\r\n",
      "/* stop CPU timer */\r\n",
      "\r\n",
      "  CUDA_CALL( cudaEventRecord( stop, 0 ) );\r\n",
      "  CUDA_CALL( cudaEventSynchronize( stop ) );\r\n",
      "  float elapsedTime;\r\n",
      "  CUDA_CALL( cudaEventElapsedTime( &elapsedTime, start, stop ) );\r\n",
      "\r\n",
      "/* print CPU timing information */\r\n",
      "\r\n",
      "  fprintf(stdout, \"Total time CPU is %f sec\\n\", elapsedTime / 1000.0f );\r\n",
      "  fprintf(stdout, \"Performance is %f GB/s\\n\", \r\n",
      "    8.0 * 2.0 * (double) size * (double) size / \r\n",
      "    ( (double) elapsedTime / 1000.0 ) * 1.e-9 );\r\n",
      "\r\n",
      "/* setup threadblock size and grid sizes */\r\n",
      "\r\n",
      "  dim3 threads( THREADS_PER_BLOCK_X, THREADS_PER_BLOCK_Y, 1 );\r\n",
      "  dim3 blocks( ( size / THREADS_PER_BLOCK_X ) + 1, \r\n",
      "               ( size / THREADS_PER_BLOCK_Y ) + 1, 1 );\r\n",
      "\r\n",
      "/* start timers */\r\n",
      "  CUDA_CALL( cudaEventRecord( start, 0 ) );\r\n",
      "\r\n",
      "/* call naive GPU transpose kernel */\r\n",
      "\r\n",
      "  naive_cuda_transpose<<< blocks, threads >>>( size, d_a, d_c );\r\n",
      "  CUDA_CHECK()\r\n",
      "  CUDA_CALL( cudaDeviceSynchronize() );\r\n",
      "\r\n",
      "/* stop the timers */\r\n",
      "\r\n",
      "  CUDA_CALL( cudaEventRecord( stop, 0 ) );\r\n",
      "  CUDA_CALL( cudaEventSynchronize( stop ) );\r\n",
      "  CUDA_CALL( cudaEventElapsedTime( &elapsedTime, start, stop ) );\r\n",
      "\r\n",
      "/* print GPU timing information */\r\n",
      "\r\n",
      "  fprintf(stdout, \"Total time GPU is %f sec\\n\", elapsedTime / 1000.0f );\r\n",
      "  fprintf(stdout, \"Performance is %f GB/s\\n\", \r\n",
      "    8.0 * 2.0 * (double) size * (double) size / \r\n",
      "    ( (double) elapsedTime / 1000.0 ) * 1.e-9 );\r\n",
      "\r\n",
      "/* copy data from device to host */\r\n",
      "\r\n",
      "  CUDA_CALL( cudaMemset( d_a, 0, numbytes ) );\r\n",
      "  CUDA_CALL( cudaMemcpy( h_a, d_c, numbytes, cudaMemcpyDeviceToHost ) );\r\n",
      "\r\n",
      "/* compare GPU to CPU for correctness */\r\n",
      "\r\n",
      "  for( int j = 0; j < size; j++ )\r\n",
      "  {\r\n",
      "    for( int i = 0; i < size; i++ )\r\n",
      "    {\r\n",
      "      if( h_c[INDX(i,j,size)] != h_a[INDX(i,j,size)] ) \r\n",
      "      {\r\n",
      "        printf(\"Error in element %d,%d\\n\", i,j );\r\n",
      "        printf(\"Host %f, device %f\\n\",h_c[INDX(i,j,size)],\r\n",
      "                                      h_a[INDX(i,j,size)]);\r\n",
      "        printf(\"FAIL\\n\");\r\n",
      "        goto end;\r\n",
      "      } /* end fi */\r\n",
      "    } /* end for i */\r\n",
      "  } /* end for j */\r\n",
      "\r\n",
      "/* free the memory */\r\n",
      "  printf(\"PASS\\n\");\r\n",
      "\r\n",
      "  end:\r\n",
      "  free( h_a );\r\n",
      "  free( h_c );\r\n",
      "  CUDA_CALL( cudaFree( d_a ) );\r\n",
      "  CUDA_CALL( cudaFree( d_c ) );\r\n",
      "  CUDA_CALL( cudaDeviceReset() );\r\n",
      "\r\n",
      "  return 0;\r\n",
      "} /* end main */\r\n"
     ]
    }
   ],
   "source": [
    "!cat exemplo7/exemplo7.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilado com Sucesso!\r\n"
     ]
    }
   ],
   "source": [
    "# Compila o exemplo7\n",
    "!nvcc -lineinfo -DDEBUG -arch=sm_30 -o exemplo7/exemplo7_out exemplo7/exemplo7.cu && echo Compilado com Sucesso!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix size is 4096\n",
      "Total memory required per matrix is 134.217728 MB\n",
      "Total time CPU is 0.292944 sec\n",
      "Performance is 0.916336 GB/s\n",
      "Total time GPU is 0.001516 sec\n",
      "Performance is 177.120579 GB/s\n",
      "PASS\n"
     ]
    }
   ],
   "source": [
    "# Executa o exemplo7\n",
    "!./exemplo7/exemplo7_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quiser gerar um zip com todos os arquivos criados, execute a célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: exemplo1/exemplo1.cu (deflated 31%)\n",
      "  adding: exemplo1/exemplo1_out (deflated 73%)\n",
      "  adding: exemplo2/exemplo2.cu (deflated 52%)\n",
      "  adding: exemplo2/exemplo2_out (deflated 73%)\n",
      "  adding: exemplo3/exemplo3.cu (deflated 64%)\n",
      "  adding: exemplo3/exemplo3_out (deflated 73%)\n",
      "  adding: exemplo4/exemplo4.cu (deflated 53%)\n",
      "  adding: exemplo4/exemplo4_out (deflated 73%)\n",
      "  adding: exemplo5/exemplo5.cu (deflated 63%)\n",
      "  adding: exemplo5/exemplo5_out (deflated 73%)\n",
      "  adding: exemplo6/exemplo6.cu (deflated 51%)\n",
      "  adding: exemplo6/exemplo6_out (deflated 73%)\n",
      "  adding: exemplo7/exemplo7.cu (deflated 65%)\n",
      "  adding: exemplo7/exemplo7_out (deflated 73%)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -f cuda_files.zip\n",
    "zip -r cuda_files.zip exemplo*/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Depois de ** executar a célula acima, você pode baixar o arquivo zip [here](cuda_files.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
