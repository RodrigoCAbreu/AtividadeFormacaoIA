{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Deep Learning Frameworks</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudo de Caso - Geração Automática de Texto com GluonNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O GluonNLP fornece implementações de modelos de Deep Learning para Processamento de Linguagem Natural. Ele foi projetado para engenheiros, pesquisadores e estudantes criarem protótipos de ideias e produtos de pesquisa com base nesses modelos e de forma rápida. De fato, há pouca programação a ser feita. Está quase tudo pronto no GluonNLP. \n",
    "\n",
    "O GluonNLP traz os principais modelos de PLN prontos para uso com pouco esforço flexibilidade e alta usabilidade.\n",
    "\n",
    "Vamos usar o GluonNLP para geração automática de texto.\n",
    "\n",
    "Visite o site oficial aqui:\n",
    "\n",
    "https://gluon-nlp.mxnet.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do Problema\n",
    "\n",
    "Este Estudo de Caso demonstra como gerar texto usando um modelo de linguagem pré-treinado das duas maneiras a seguir:\n",
    "\n",
    "- Com amostrador de sequência (sequence sampler)\n",
    "- Com amostrador de busca por feixe (beam search sampler)\n",
    "\n",
    "Variáveis a serem configuradas ao gerar sequências:\n",
    "\n",
    "- V = tamanho do vocabulário\n",
    "- T = comprimento da sequência\n",
    "- V ^ T = o número de resultados possíveis para considerar uma sequência.\n",
    "\n",
    "Dado um modelo de linguagem, podemos gerar sequências de acordo com a probabilidade de ocorrerem. A cada etapa do tempo, um modelo de linguagem prediz a probabilidade de cada palavra ocorrer, considerando o contexto das etapas anteriores. As saídas a qualquer momento podem ser qualquer palavra do vocabulário cujo tamanho é V e, portanto, o número de todos os resultados possíveis para uma sequência de comprimento T é, portanto, V ^ T.\n",
    "\n",
    "Embora algumas vezes desejemos gerar sentenças de acordo com a probabilidade de ocorrência, outras vezes desejamos encontrar as sentenças que **têm maior probabilidade de ocorrer**. Isso é especialmente verdade no caso da tradução de idiomas, onde não queremos apenas ver uma tradução qualquer. Queremos a melhor tradução. Embora encontrar o resultado ideal rapidamente se torne intratável com o aumento do tempo, ainda existem muitas maneiras de gerar sequências razoavelmente boas. O GluonNLP fornece dois amostradores para gerar texto a partir de um modelo de linguagem: SequenceSampler e BeamSearchSampler. Usaremos ambos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "XpYiCxWFwrdF",
    "outputId": "2c3764c9-ba39-4b61-ad10-e82043c7963e"
   },
   "outputs": [],
   "source": [
    "# Instalamos o MXNET (não é necessário suporte a GPU)\n",
    "!pip install -q mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "colab_type": "code",
    "id": "O45K03dixX4N",
    "outputId": "930e2fd6-eede-4225-df08-459dded10f3c"
   },
   "outputs": [],
   "source": [
    "# Agora instalamos o GluonNLP\n",
    "!pip install -q gluonnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "85aYYwuSIBaM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmpm/opt/anaconda3/lib/python3.7/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  Optimizer.opt_registry[name].__name__))\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "\n",
    "# Esse pacote foi baixado do repositório do GluonNLP e está sendo fornecido a você junto com este Jupyter Notebook\n",
    "# https://github.com/dmlc/gluon-nlp\n",
    "import text_generation.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy    1.18.4\n",
      "mxnet    1.6.0\n",
      "gluonnlp 0.8.1\n",
      "Data Science Academy\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o Modelo Pré-Treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos alterar o dispositivo para CPU (GPU não é necessário neste estudo de caso)\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos importar o modelo pré-treinado para geração de texto\n",
    "model, vocab = text_generation.model.get_model(name = 'gpt2_117m',\n",
    "                                               dataset_name = 'openai_webtext',\n",
    "                                               pretrained = True,\n",
    "                                               ctx = ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criamos então o tokenizador\n",
    "tokenizer = nlp.data.GPT2BPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E também o objeto para remover a tokenização (usaremos para mostrar o texto gerado)\n",
    "detokenizer = nlp.data.GPT2BPEDetokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Definimos o token de final de texto\n",
    "eos_id = vocab[vocab.eos_token]\n",
    "print(vocab.eos_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Sampler\n",
    "\n",
    "Um SequenceSampler gera amostras da distribuição multinomial contextual produzida pelo modelo de linguagem a cada etapa do tempo. Podemos usar a opção de \"temperatura\" no SequenceSampler, que controla a \"temperatura\" da função softmax. Temperatura aqui nada mais é do que a intensidade, mas é assim que o parâmetro é chamado.\n",
    "\n",
    "Para cada entrada igual, o Sequence Sampler pode amostrar várias sequências independentes de uma só vez. O número de sequências independentes a serem amostradas pode ser especificado através do argumento beam_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta string será usada como ponto de partida para a geração de texto\n",
    "bos_str = 'Deep learning and natural language processing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionamos um espaço no início da string\n",
    "if not bos_str.startswith(' '):\n",
    "    bos_str = ' ' + bos_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizamos a string\n",
    "bos_tokens = tokenizer(bos_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ĠDeep', 'Ġlearning', 'Ġand', 'Ġnatural', 'Ġlanguage', 'Ġprocessing']\n"
     ]
    }
   ],
   "source": [
    "# Geramos o vocabulário com os tokens\n",
    "bos_ids = vocab[bos_tokens]\n",
    "print(bos_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora definimos o decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe para o decoder\n",
    "class GPT2Decoder(text_generation.model.LMDecoder):\n",
    "    def __call__(self, inputs, states):\n",
    "        \n",
    "        # Recebe os inputs\n",
    "        inputs = inputs.expand_dims(axis = 1)\n",
    "        \n",
    "        # Gera as saídas\n",
    "        out, new_states = self.net(inputs, states)\n",
    "        \n",
    "        # Reshape das saídas\n",
    "        out = mx.nd.slice_axis(out, axis = 1, begin = 0, end = 1).reshape((inputs.shape[0], -1))\n",
    "        \n",
    "        return out, new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o objeto\n",
    "decoder = GPT2Decoder(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E precisamos definir o estado inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para o estado inicial\n",
    "def get_initial_input_state(decoder, bos_ids, temperature):\n",
    "    \n",
    "    # Inputs e estado inicial\n",
    "    inputs, begin_states = decoder.net(mx.nd.array([bos_ids], dtype = np.int32, ctx = ctx), None)\n",
    "    \n",
    "    # Reshape dos inputs\n",
    "    inputs = inputs[:, -1, :]\n",
    "    \n",
    "    # Probabilidades (observe o parâmetro de temperatura)\n",
    "    smoothed_probs = (inputs / temperature).softmax(axis = 1)\n",
    "    \n",
    "    # Amostra multidimensional\n",
    "    inputs = mx.nd.sample_multinomial(smoothed_probs, dtype = np.int32)\n",
    "    \n",
    "    return inputs, begin_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros do modelo\n",
    "beam_size = 2\n",
    "temperature = 0.97\n",
    "num_results = 2\n",
    "max_len = 256 - len(bos_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o sampler\n",
    "sampler = nlp.model.SequenceSampler(beam_size = beam_size,\n",
    "                                    decoder = decoder,\n",
    "                                    eos_id = eos_id,\n",
    "                                    max_length = max_len,\n",
    "                                    temperature = temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para geração de texto\n",
    "def generate(decoder, bos_ids, temperature, sampler, num_results, vocab):\n",
    "    \n",
    "    # Inputs e estado inicial\n",
    "    inputs, begin_states = get_initial_input_state(decoder, bos_ids, temperature)\n",
    "    \n",
    "    # Amostras, escores e comprimentos válidos\n",
    "    samples, scores, valid_lengths = sampler(inputs, begin_states)\n",
    "    \n",
    "    # Converte amostras, scores e comprimentos válidos para o formato numpy\n",
    "    samples = samples[0].asnumpy()\n",
    "    scores = scores[0].asnumpy()\n",
    "    valid_lengths = valid_lengths[0].asnumpy()\n",
    "\n",
    "    # Resultado\n",
    "    print('\\nResultado Gerado:\\n')\n",
    "    for i in range(num_results):\n",
    "        \n",
    "        # Gera os tokens (novo texto)\n",
    "        generated_tokens = [vocab.idx_to_token[ele] for ele in samples[i][:valid_lengths[i]]]\n",
    "        \n",
    "        # Adiciona os tokens gerados ao texto inicial\n",
    "        tokens = bos_tokens + generated_tokens[1:]\n",
    "        \n",
    "        # Desfaz a tokenização para mostrar o resultado no formato de texto\n",
    "        print([detokenizer(tokens).strip(), scores[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultado Gerado:\n",
      "\n",
      "[\"Deep learning and natural language processing serious improvements over existing programming languages.\\n\\nNo framework or language developers have yet heard about the credentials of Cobolite's hardware manager but the potential of OpenCL/TODO for coding should seem unlimited.<|endoftext|>\", -180.59767]\n",
      "[\"Deep learning and natural language processing steps that make them easier to define than games or programs. Students with a BPU school require rigorous tutorial training that can allow them to code with their native language and to dig deeper into neural networks. The combination of software practice and instilled knowledge will allow instructors to develop the skills that will help them to highly perform their development tasks. Building a STEM projects lasting through formal and clinical work and into their career. Women's education has the basic undergraduate competencies. Opposing field based in STEM, applied... Learn mathematics. Midstream mentoring expands all majors would need because it valid often 1000 hours at so hormones as any degree, many fields of applied teaching. There are college leadership training over years path the training methods during middle to integrate maximum attrition. Apply to ensure the scholar youth base work. Practically earn 700 1200 hours, many courses. Programing and the courses to hold enrollment\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nunder last lead to even grades must try to life. As for advanced degree\\nI HEXCELEEAR CARE\\nLong time, Women's already students highly fewer behavioral issue updates\\nUnderstanding active was waived straight or its modern.\\n2016 4 years\\nAugmented\\nUnder UMS can break\\nseures 3<|endoftext|>\", -1243.1415]\n"
     ]
    }
   ],
   "source": [
    "# Executa o gerador de texto\n",
    "generate(decoder, bos_ids, temperature, sampler, num_results, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texto gerado com sucesso. Vejamos se conseguimos melhorar a performance mudando o sampler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search Sampler\n",
    "\n",
    "Para superar a complexidade exponencial na decodificação de sequência, a pesquisa por feixe faz uma decodificação mais intensa, mantendo as sequências que provavelmente são baseadas na probabilidade até a etapa de tempo atual. \n",
    "\n",
    "O tamanho desse subconjunto é chamado de tamanho do feixe (beam size). \n",
    "\n",
    "Suponha que o tamanho do feixe seja K e o tamanho do vocabulário de saída seja V. Ao selecionar os feixes a serem mantidos, o algoritmo Beam Search primeiro prediz todas as possíveis palavras sucessoras dos feixes K anteriores, cada um com V saídas possíveis. Isso se torna um total de caminhos K * V. Desses caminhos K * V, a pesquisa por feixe os classifica por sua pontuação, mantendo apenas os caminhos K principais.\n",
    "\n",
    "O BeamSearchScorer é um HybridBlock simples que implementa a função de pontuação com penalidade de comprimento no artigo do Google NMT:\n",
    "\n",
    "scores = (log_probs + scores) / length_penalty\n",
    "\n",
    "length_penalty = (K + length)^alpha / (K + 1)^alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o scorer, que vai definir a intensidade da decodificação\n",
    "scorer = nlp.model.BeamSearchScorer(alpha = 0, K = 5, from_logits = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o sampler\n",
    "beam_sampler = nlp.model.BeamSearchSampler(beam_size = 3,\n",
    "                                           decoder = decoder,\n",
    "                                           eos_id = eos_id,\n",
    "                                           scorer = scorer,\n",
    "                                           max_length = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultado Gerado:\n",
      "\n",
      "['Deep learning and natural language processing\\n\\nThe study was published in the journal Proceedings of the National Academy of Sciences.<|endoftext|>', -13.526959]\n",
      "['Deep learning and natural language processing\\n\\nThe study was published in the journal Proceedings of the National Academy of Sciences.\\n\\nExplore further: Researchers discover a new way to learn about the brain\\n\\n\\nMore information: \"A new way to learn about the brain: A new way to learn about the brain,\" Proceedings of the National Academy of Sciences, DOI: 10.10.10.10731701/pnas.1701221709617410<|endoftext|>', -91.53121]\n"
     ]
    }
   ],
   "source": [
    "# Gera o texto\n",
    "generate(decoder, bos_ids, temperature, beam_sampler, num_results, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hummm...melhorou de forma considerável. Talvez um ajuste dos hiperparâmetros deixe o resultado ainda melhor, mas isso agora é com você."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "3. Extended_Forecasting_Tutorial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
